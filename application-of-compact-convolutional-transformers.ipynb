{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/matthewjansen/application-of-compact-convolutional-transformers?scriptVersionId=143293622\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<a id=toc></a>\n<h1 style=\"padding: 35px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/V6fJnWTt/Ali.png); background-size: 100% auto;background-position: 0px 500px; \n\"><span style='color:white'>Application of Compact Convolutional Transformers</span></h1>\n\n<br>\n\n<center>\n    <figure>\n        <img src=\"https://production-media.paperswithcode.com/methods/cct_sBBD6Sv.png\" alt =\"Skin Cancer\" style='width:70%;'>\n        <figcaption>\n            Image Source: <a href=\"https://paperswithcode.com/method/cct\">PapersWithCode | Compact Convolutional Transformers</a>\n        </figcaption>\n    </figure>\n</center>\n\n<br>\n\n## üìÑ Overview\n\nIn recent years, Transformers have become the go-to for Natural Language Processing (NLP) and have been used in research for Computer Vision and Time Series Forecasting to make groundbreaking discoveries.\n\nThere is a clear association between the increasing number of trainable parameters in models and the progress made in these domains, and many researchers may believe this is due to the inability of transformers to adapt to small datasets.\n\nThe Compact Convolutional Transformer approach introduced in the paper \"Escaping the Big Data Paradigm with Compact Transformers\" addresses the issues with Transformer models on small datasets. \n\nIn this notebook, we will cover a comprehensive guide on how to implement and train a CCT model that is specifically designed for classifying skin cancer. The model will accurately identify potential skin cancer cases, which will promote early intervention and improve patient outcomes.\n\n> **Paper:** [Escaping the Big Data Paradigm with Compact Transformers](https://arxiv.org/abs/2104.05704v4)\n\n<br>\n\n## üìÅ Dataset\n\nThe dataset used in this notebook is know as International Skin Imaging Collaboration (ISIC) Skin Cancer Detection Dataset. The ISIC is an academia and industry partnership designed to facilitate the application of digital skin imaging to help reduce melanoma mortality. This notebook uses the Kaggle version of this dataset.\n\n**For more information check the following:**\n> - [Kaggle Dataset](https://www.kaggle.com/datasets/fanconic/skin-cancer-malignant-vs-benign)\n> - [The International Skin Imaging Collaboration](https://www.isic-archive.com/#!/topWithHeader/wideContentTop/main)\n\n<br>\n\n## ‚öïÔ∏è What is Skin Cancer?\n\nSkin cancer is the abnormal growth of skin cells and most often develops on skin exposed to the sun. This common form of cancer can also occur on areas of your skin not ordinarily exposed to sunlight.\n\nCancer begins when healthy cells change and grow out of control, forming a mass called a tumor. A tumor can be cancerous or benign. Cancerous tumor are malignant, meaning they can grow and spread to other parts of the body. Benign tumors can become quite large, but they will not invade nearby tissue or spread to other parts of your body. Benign tumors have distinct, smooth, regular borders, while malignant tumors have irregular borders and grows faster than benign tumors.\n\n> **Read here for more:** [Skin Cancer (Non-Melanoma) Guide](https://www.cancer.net/cancer-types/skin-cancer-non-melanoma/introduction#:~:text=About%20skin%20cancer&text=A%20cancerous%20tumor%20is%20malignant,most%20common%20type%20of%20cancer.)\n\n<br>\n\n## üîÑ Previous Works\n\nI've covered the classification of skin cancer in depth in this notebook titled: [Transfer Learning | Skin Cancer Classification](https://www.kaggle.com/code/matthewjansen/transfer-learning-skin-cancer-classification/notebook). In the notebook, the usage of basic convolutional neural networks, along with the application of transfer learning techniques are covered and discussed in detail. The utilisation of State-of-the-Art (SOTA) models such as EfficientNet V2 B0 and the Vision Transformer B16 for transfer learning are also included.\n\nThis notebook reuses some of the code from the afformentioned notebook. If you are interested in my previous works, feel free to visit the notebook.\n\n<br>\n\n## üìù Note\nI'd like to thank [@jaxford](https://www.kaggle.com/jaxford) for recommending a project based on this paper. To you, the reader: please feel free to **recommend any research papers** you may find interesting as I am keen on reading up on the literature and, maybe (not garenteed), writing a notebook covering their implementation. \n\nThank you for your time! \n\n<br>\n\n<hr>\n\n## Table of contents\n- [1 | Inspecting The Dataset](#1)\n   > - [Get image paths with glob](#1.1)\n   > - [View the number of images present in the dataset](#1.2)\n   > - [Create Pandas DataFrames for paths and labels](#1.3)\n   > - [Load & View Random Sample Image](#1.4)\n   > - [View multiple random samples](#1.5)\n   > - [View Train Labels Distribution](#1.6)\n  \n- [2 | Preprocessing: Building An Input Data Pipeline](#2)\n   > - [Create Train & Validation Splits](#2.1)\n   > - [View New Train & Validation Labels Distribution](#2.2)\n   > - [Create an Image Data Augmentation Layer](#2.3)\n   > - [Create Input Data Pipeline w. tf.data API](#2.4)\n   \n- [3 | The Compact Convolutional Transformers](#3)\n   > - [Stochastic Depth](#3.1)\n   > - [Multi-Layer Perceptron (MLP)](#3.2notebook)\n   > - [Generate CCT Model](#3.3)\n   \n- [4 | Training The Compact Convolutional Transformer Model](#4)\n\n   \n- [5 | Performance Evaluation](#5)\n   > - [View Model Histories](#5.1)\n   > - [Plot Confusion Matrix](#5.2)\n   > - [Plot ROC Curves](#5.3)\n   > - [Inpsect Classification Reports](#5.4)\n   > - [Record Classification Metrics](#5.5)\n   \n- [Conclusion](#conclusion)","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:29:43.820896Z","iopub.execute_input":"2023-09-17T10:29:43.821245Z","iopub.status.idle":"2023-09-17T10:29:45.205642Z","shell.execute_reply.started":"2023-09-17T10:29:43.821215Z","shell.execute_reply":"2023-09-17T10:29:45.203701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport os\nimport glob\nimport time\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Sequential\nfrom tensorflow.keras.utils import plot_model\nimport tensorflow_addons as tfa\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    classification_report, precision_recall_fscore_support,\n    accuracy_score, f1_score, matthews_corrcoef,\n    confusion_matrix, ConfusionMatrixDisplay\n)\nfrom scikitplot.metrics import plot_roc","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:29:45.213185Z","iopub.execute_input":"2023-09-17T10:29:45.214794Z","iopub.status.idle":"2023-09-17T10:29:55.607383Z","shell.execute_reply.started":"2023-09-17T10:29:45.214755Z","shell.execute_reply":"2023-09-17T10:29:55.606391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    POSITIONAL_EMBEDDING = True\n    TOKENIZER_CONV_LAYERS = 2\n    PROJECTION_DIM = 128\n    NUM_HEADS = 2\n    \n    TRANSFORMER_UNITS = [\n        PROJECTION_DIM,\n        PROJECTION_DIM,\n    ]\n    \n    TRANSFORMER_LAYERS = 4\n    STOCHASTIC_DEPTH_RATE = 0.1\n    NUM_CLASSES = 2\n\n    LEARNING_RATE = 0.001\n    WEIGHT_DECAY = 0.0001\n    EPOCHS = 60\n    BATCH_SIZE = 64\n    SEED = 42\n    TF_SEED = 768\n    \n    #######################################################\n    # Note: If you wish to increase the image size beyond # \n    #       the current value, make sure you have access  # \n    #       to enough memory.                             #\n    #######################################################\n    HEIGHT = 144\n    WIDTH = 144\n    CHANNELS = 3\n    IMAGE_SIZE = 144\n    IMAGE_SHAPE = (144, 144, 3)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:29:55.609017Z","iopub.execute_input":"2023-09-17T10:29:55.609848Z","iopub.status.idle":"2023-09-17T10:29:55.616894Z","shell.execute_reply.started":"2023-09-17T10:29:55.609798Z","shell.execute_reply":"2023-09-17T10:29:55.615406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='1'></a>\n# 1 | Inspecting The Dataset\n<div style=\"padding: 4px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/V6fJnWTt/Ali.png); background-size: 100% auto;\"></div>","metadata":{}},{"cell_type":"code","source":"# Define paths\nDATASET_PATH = \"/kaggle/input/skin-cancer-malignant-vs-benign/\"\nTRAIN_PATH = '/kaggle/input/skin-cancer-malignant-vs-benign/train/'\nTEST_PATH = '/kaggle/input/skin-cancer-malignant-vs-benign/test/'","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:29:55.620651Z","iopub.execute_input":"2023-09-17T10:29:55.621397Z","iopub.status.idle":"2023-09-17T10:29:55.627181Z","shell.execute_reply.started":"2023-09-17T10:29:55.621358Z","shell.execute_reply":"2023-09-17T10:29:55.626223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate a summary of the dataset\nprint('DATASET SUMMARY')\nprint('========================\\n')\nfor dirpath, dirnames, filenames in os.walk(DATASET_PATH):\n    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in {dirpath}\")\nprint('\\n========================')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:29:55.629084Z","iopub.execute_input":"2023-09-17T10:29:55.629505Z","iopub.status.idle":"2023-09-17T10:29:58.809591Z","shell.execute_reply.started":"2023-09-17T10:29:55.629473Z","shell.execute_reply":"2023-09-17T10:29:58.808589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='1.1'></a>\n### Get image paths with glob","metadata":{}},{"cell_type":"code","source":"%%time\ntrain_images = glob.glob(f\"{TRAIN_PATH}**/*.jpg\")\ntest_images = glob.glob(f\"{TEST_PATH}**/*.jpg\")","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:29:58.810837Z","iopub.execute_input":"2023-09-17T10:29:58.811213Z","iopub.status.idle":"2023-09-17T10:29:58.835952Z","shell.execute_reply.started":"2023-09-17T10:29:58.811176Z","shell.execute_reply":"2023-09-17T10:29:58.835045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='1.2'></a>\n### View the number of images present in the dataset","metadata":{}},{"cell_type":"code","source":"# Get train & test set sizes\ntrain_size = len(train_images)\ntest_size = len(test_images)\n\n# Get dataset size\ntotal = train_size + test_size\n\n# View samples counts\nprint(f'train samples count:\\t\\t{train_size}')\nprint(f'test samples count:\\t\\t{test_size}')\nprint('=======================================')\nprint(f'TOTAL:\\t\\t\\t\\t{total}')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:29:58.838526Z","iopub.execute_input":"2023-09-17T10:29:58.839158Z","iopub.status.idle":"2023-09-17T10:29:58.845907Z","shell.execute_reply.started":"2023-09-17T10:29:58.83912Z","shell.execute_reply":"2023-09-17T10:29:58.844794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='1.3'></a>\n### Create Pandas DataFrames for paths and labels","metadata":{}},{"cell_type":"code","source":"def generate_labels(image_paths):\n    return [_.split('/')[-2:][0] for _ in image_paths]\n\n\ndef build_df(image_paths, labels):\n    # Create dataframe\n    df = pd.DataFrame({\n        'image_path': image_paths,\n        'label': generate_labels(labels)\n    })\n    \n    # Generate label encodings\n    df['label_encoded'] = df.apply(lambda row: 0 if row.label == 'malignant' else 1, axis=1)\n    \n    # Shuffle and return df\n    return df.sample(frac=1, random_state=CFG.SEED).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:29:58.847725Z","iopub.execute_input":"2023-09-17T10:29:58.848227Z","iopub.status.idle":"2023-09-17T10:29:58.856038Z","shell.execute_reply.started":"2023-09-17T10:29:58.848192Z","shell.execute_reply":"2023-09-17T10:29:58.854912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build the DataFrames\ntrain_df = build_df(train_images, generate_labels(train_images))\ntest_df = build_df(test_images, generate_labels(test_images))","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:29:58.857878Z","iopub.execute_input":"2023-09-17T10:29:58.858352Z","iopub.status.idle":"2023-09-17T10:29:58.933387Z","shell.execute_reply.started":"2023-09-17T10:29:58.858316Z","shell.execute_reply":"2023-09-17T10:29:58.932459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View first 5 samples in the training set\ntrain_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:29:58.937875Z","iopub.execute_input":"2023-09-17T10:29:58.938147Z","iopub.status.idle":"2023-09-17T10:29:58.955439Z","shell.execute_reply.started":"2023-09-17T10:29:58.938123Z","shell.execute_reply":"2023-09-17T10:29:58.954509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='1.4'></a>\n### Load & View Random Sample Image","metadata":{}},{"cell_type":"code","source":"def _load(image_path):\n    # Read and decode an image file to a uint8 tensor\n    image = tf.io.read_file(image_path)\n    image = tf.io.decode_jpeg(image, channels=3)\n    \n    # Resize image\n    image = tf.image.resize(image, [CFG.HEIGHT, CFG.WIDTH],\n                            method=tf.image.ResizeMethod.LANCZOS5)\n    \n    # Convert image dtype to float32 and NORMALIZE!!!\n    image = tf.cast(image, tf.float32)/255.\n    \n    # Return image\n    return image\n\ndef view_sample(image, label, color_map='rgb', fig_size=(8, 10)):\n    plt.figure(figsize=fig_size)\n    \n    if color_map=='rgb':\n        plt.imshow(image)\n    else:\n        plt.imshow(tf.image.rgb_to_grayscale(image), cmap=color_map)\n    \n    plt.title(f'Label: {label}', fontsize=16)\n    return","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:29:58.95686Z","iopub.execute_input":"2023-09-17T10:29:58.957388Z","iopub.status.idle":"2023-09-17T10:29:58.967059Z","shell.execute_reply.started":"2023-09-17T10:29:58.957353Z","shell.execute_reply":"2023-09-17T10:29:58.965953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select random sample from train_df\nidx = random.sample(train_df.index.to_list(), 1)[0]\n\n# Load the random sample and label\nsample_image, sample_label = _load(train_df.image_path[idx]), train_df.label[idx]\n\n# View the random sample\nview_sample(sample_image, sample_label, color_map='inferno')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:29:58.968888Z","iopub.execute_input":"2023-09-17T10:29:58.969395Z","iopub.status.idle":"2023-09-17T10:30:04.323402Z","shell.execute_reply.started":"2023-09-17T10:29:58.969362Z","shell.execute_reply":"2023-09-17T10:30:04.322485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='1.5'></a>\n### View multiple random samples","metadata":{}},{"cell_type":"code","source":"def view_mulitiple_samples(df, sample_loader, count=10, color_map='rgb', fig_size=(14, 10)):\n    rows = count//5\n    if count%5 > 0:\n        rows +=1\n    \n    idx = random.sample(df.index.to_list(), count)    \n    fig = plt.figure(figsize=fig_size)\n\n    for column, _ in enumerate(idx):\n        plt.subplot(rows, 5, column+1)\n        plt.title(f'Label: {df.label[_]}', pad=20)\n        \n        if color_map=='rgb':\n            plt.imshow(sample_loader(df.image_path[_]))\n        else:\n            plt.imshow(tf.image.rgb_to_grayscale(sample_loader(df.image_path[_])), cmap=color_map)\n    \n    return\n\nview_mulitiple_samples(train_df, _load, \n                       count=25, color_map='inferno', \n                       fig_size=(20, 24))","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:30:04.324416Z","iopub.execute_input":"2023-09-17T10:30:04.324751Z","iopub.status.idle":"2023-09-17T10:30:13.944498Z","shell.execute_reply.started":"2023-09-17T10:30:04.324725Z","shell.execute_reply":"2023-09-17T10:30:13.943602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='1.6'></a>\n### View Train Labels Distribution","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, figsize=(14, 10))\n\n# Set the spacing between subplots\nfig.tight_layout(pad=6.0)\n\n# Plot Train Labels Distribution\nax1.set_title('Train Labels Distribution', fontsize=20, pad=20)\ntrain_distribution = train_df['label'].value_counts().sort_values()\nsns.barplot(x=train_distribution.values,\n            y=list(train_distribution.keys()),\n            orient=\"h\",\n            ax=ax1)\n\n# Plot Test Labels Distribution\nax2.set_title('Test Labels Distribution', fontsize=20, pad=20)\ntest_distribution = test_df['label'].value_counts().sort_values()\nsns.barplot(x=test_distribution.values,\n            y=list(test_distribution.keys()),\n            orient=\"h\",\n            ax=ax2);\n\nsns.despine();","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:30:13.946051Z","iopub.execute_input":"2023-09-17T10:30:13.946678Z","iopub.status.idle":"2023-09-17T10:30:14.757791Z","shell.execute_reply.started":"2023-09-17T10:30:13.946645Z","shell.execute_reply":"2023-09-17T10:30:14.756718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n    <h3>Observe</h3>\n    We see that both the training and test sets are fairly balanced. However, we'll need to create a validation set for hyperparameter tuning.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<center><div style='color:#ffffff;\n           display:inline-block;\n           padding: 5px 5px 5px 5px;\n           border-radius:5px;\n           background-color:#B6EADA;\n           font-size:100%;'><a href=#toc style='text-decoration: none; color:#03001C;'>‚¨ÜÔ∏è Back To Top</a></div></center>\n\n<a id='2'></a>\n# 2 | Preprocessing: Building An Input Data Pipeline\n<div style=\"padding: 4px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/V6fJnWTt/Ali.png); background-size: 100% auto;\"></div>\n\n<br>\n\nIn order to train models with this dataset we'll build an input data pipeline using TensorFlow's tf.data API to handle the loading and passing of the image data to the model. \n\nTo achieve a faster training time we'll insure that the training data is batched and prefetched while the model is training on a previously passed sample. \n\n> For more information on the tf.data API, follow this link: [Better performance with the tf.data API](https://www.tensorflow.org/guide/data_performance)\n\nSince ImageDataGenerators aren't used in this notebook, an image data augmentation layer will have to be constructed manually. \n\n> For more information on ImageDataGenerators, follow these links: \n> \n> - [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)\n> - [tf.data: Build TensorFlow input pipelines](https://www.tensorflow.org/guide/data)","metadata":{}},{"cell_type":"markdown","source":"<a id='2.1'></a>\n### Create Train & Validation Splits","metadata":{}},{"cell_type":"code","source":"# Create Train/Val split with Training Set\ntrain_split_idx, val_split_idx, _, _ = train_test_split(train_df.index, \n                                                        train_df.label_encoded, \n                                                        test_size=0.15,\n                                                        stratify=train_df.label_encoded,\n                                                        random_state=CFG.SEED)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:30:14.762107Z","iopub.execute_input":"2023-09-17T10:30:14.76278Z","iopub.status.idle":"2023-09-17T10:30:14.775321Z","shell.execute_reply.started":"2023-09-17T10:30:14.762744Z","shell.execute_reply":"2023-09-17T10:30:14.774339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get new training and validation data\ntrain_new_df = train_df.iloc[train_split_idx].reset_index(drop=True)\nval_df = train_df.iloc[val_split_idx].reset_index(drop=True)\n\n# View shapes\ntrain_new_df.shape, val_df.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:30:14.779904Z","iopub.execute_input":"2023-09-17T10:30:14.782343Z","iopub.status.idle":"2023-09-17T10:30:14.796155Z","shell.execute_reply.started":"2023-09-17T10:30:14.782306Z","shell.execute_reply":"2023-09-17T10:30:14.795031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2.2'></a>\n### View New Train & Validation Labels Distribution","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, figsize=(14, 10))\n\n# Set the spacing between subplots\nfig.tight_layout(pad=6.0)\n\n# Plot New Train Labels Distribution\nax1.set_title('New Train Labels Distribution', fontsize=20, pad=20)\ntrain_new_distribution = train_new_df['label'].value_counts().sort_values()\nsns.barplot(x=train_new_distribution.values,\n            y=list(train_new_distribution.keys()),\n            orient=\"h\",\n            ax=ax1)\n\n# Plot Validation Labels Distribution\nax2.set_title('Validation Labels Distribution', fontsize=20, pad=20)\nval_distribution = val_df['label'].value_counts().sort_values()\nsns.barplot(x=val_distribution.values,\n            y=list(val_distribution.keys()),\n            orient=\"h\",\n            ax=ax2);\n\nsns.despine();","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:30:14.797379Z","iopub.execute_input":"2023-09-17T10:30:14.797941Z","iopub.status.idle":"2023-09-17T10:30:15.40792Z","shell.execute_reply.started":"2023-09-17T10:30:14.797909Z","shell.execute_reply":"2023-09-17T10:30:15.40688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2.3'></a>\n### Create an Image Data Augmentation Layer","metadata":{}},{"cell_type":"code","source":"# Build augmentation layer\naugmentation_layer = Sequential([\n    layers.RandomFlip(mode='horizontal_and_vertical', seed=CFG.TF_SEED),\n    layers.RandomZoom(height_factor=(-0.1, 0.1), width_factor=(-0.1, 0.1), seed=CFG.TF_SEED),\n], name='augmentation_layer')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:30:15.412263Z","iopub.execute_input":"2023-09-17T10:30:15.414528Z","iopub.status.idle":"2023-09-17T10:30:15.444219Z","shell.execute_reply.started":"2023-09-17T10:30:15.41449Z","shell.execute_reply":"2023-09-17T10:30:15.443287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = tf.image.rgb_to_grayscale(sample_image)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 10))\n\n# Set the spacing between subplots\nfig.tight_layout(pad=6.0)\n\n# View Original Image\nax1.set_title('Original Image', fontsize=20, pad=20)\nax1.imshow(image, cmap='inferno');\n\n# View Augmented Image\nax2.set_title('Augmented Image', fontsize=20, pad=20)\nax2.imshow(augmentation_layer(image), cmap='inferno');","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:30:15.448718Z","iopub.execute_input":"2023-09-17T10:30:15.450969Z","iopub.status.idle":"2023-09-17T10:30:17.127729Z","shell.execute_reply.started":"2023-09-17T10:30:15.450934Z","shell.execute_reply":"2023-09-17T10:30:17.126579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n    <h3>Observe</h3>\n    The augmentation layer generates augmentations that are slightly different to the original images. This is intended as we aim to generate augmented images which are similar to that of the original dataset images while preserving the images key features. \n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id='2.4'></a>\n### Create Input Data Pipeline w. tf.data API","metadata":{}},{"cell_type":"code","source":"def encode_labels(labels, encode_depth=2):\n    return tf.one_hot(labels, depth=encode_depth).numpy()\n\ndef create_pipeline(df, load_function, augment=False, batch_size=32, shuffle=False, cache=None, prefetch=False):\n    '''\n    Generates an input pipeline using the tf.data API given a Pandas DataFrame and image loading function.\n    \n    @params\n        - df: (pd.DataFrame) -> DataFrame containing paths and labels \n        - load_function: (function) -> function used to load images given their paths\n        - augment: (bool) -> condition for applying augmentation\n        - batch_size: (int) -> size for batched (default=32) \n        - shuffle: (bool) -> condition for data shuffling, data is shuffled when True (default=False)\n        - cache: (str) -> cache path for caching data, data is not cached when None (default=None)\n        - prefetch: (bool) -> condition for prefeching data, data is prefetched when True (default=False)\n        \n    @returns\n        - dataset: (tf.data.Dataset) -> dataset input pipeline used to train a TensorFlow model\n    '''\n    # Get image paths and labels from DataFrame\n    image_paths = df.image_path\n    image_labels = encode_labels(df.label_encoded)\n    AUTOTUNE = tf.data.AUTOTUNE\n    \n    # Create dataset with raw data from DataFrame\n    ds = tf.data.Dataset.from_tensor_slices((image_paths, image_labels))\n    \n    # Map augmentation layer and load function to dataset inputs if augment is True\n    # Else map only the load function\n    if augment:\n        ds = ds.map(lambda x, y: (augmentation_layer(load_function(x)), y), num_parallel_calls=AUTOTUNE)\n    else:\n        ds = ds.map(lambda x, y: (load_function(x), y), num_parallel_calls=AUTOTUNE)\n    \n    # Apply shuffling based on condition\n    if shuffle:\n        ds = ds.shuffle(buffer_size=1000)\n        \n    # Apply batching\n    ds = ds.batch(batch_size)\n    \n    # Apply caching based on condition\n    # Note: Use cache in memory (cache='') if the data is small enough to fit in memory!!!\n    if cache != None:\n        ds = ds.cache(cache)\n    \n    # Apply prefetching based on condition\n    # Note: This will result in memory trade-offs\n    if prefetch:\n        ds = ds.prefetch(buffer_size=AUTOTUNE)\n    \n    # Return the dataset\n    return ds","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:30:17.131328Z","iopub.execute_input":"2023-09-17T10:30:17.132023Z","iopub.status.idle":"2023-09-17T10:30:17.149001Z","shell.execute_reply.started":"2023-09-17T10:30:17.131987Z","shell.execute_reply":"2023-09-17T10:30:17.14807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate Train Input Pipeline\ntrain_ds = create_pipeline(train_new_df, _load, augment=True, \n                           batch_size=CFG.BATCH_SIZE, \n                           shuffle=False, prefetch=True)\n\n# Generate Validation Input Pipeline\nval_ds = create_pipeline(val_df, _load, \n                         batch_size=CFG.BATCH_SIZE, \n                         shuffle=False, prefetch=False)\n\n# Generate Test Input Pipeline\ntest_ds = create_pipeline(test_df, _load, \n                          batch_size=CFG.BATCH_SIZE, \n                          shuffle=False, prefetch=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:30:17.154155Z","iopub.execute_input":"2023-09-17T10:30:17.157262Z","iopub.status.idle":"2023-09-17T10:30:17.798064Z","shell.execute_reply.started":"2023-09-17T10:30:17.157224Z","shell.execute_reply":"2023-09-17T10:30:17.797056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View string representation of datasets\nprint('========================================')\nprint('Train Input Data Pipeline:\\n\\n', train_ds)\nprint('========================================')\nprint('Validation Input Data Pipeline:\\n\\n', val_ds)\nprint('========================================')\nprint('Test Input Data Pipeline:\\n\\n', test_ds)\nprint('========================================')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:30:17.799802Z","iopub.execute_input":"2023-09-17T10:30:17.800482Z","iopub.status.idle":"2023-09-17T10:30:17.809414Z","shell.execute_reply.started":"2023-09-17T10:30:17.800447Z","shell.execute_reply":"2023-09-17T10:30:17.808296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center><div style='color:#ffffff;\n           display:inline-block;\n           padding: 5px 5px 5px 5px;\n           border-radius:5px;\n           background-color:#B6EADA;\n           font-size:100%;'><a href=#toc style='text-decoration: none; color:#03001C;'>‚¨ÜÔ∏è Back To Top</a></div></center>\n\n<a id=3></a>\n# 3 | The Compact Convolutional Transformers\n<div style=\"padding: 4px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/V6fJnWTt/Ali.png); background-size: 100% auto;\"></div>\n\n<br>\n\nCompact Convolutional Transformers (CCTs) represent a novel hybrid architectural approach which involves merging the benefits of Convolutional Neural Networks and Transformers. This novel architecture efficiently captures local patterns and global dependencies within small datasets.\n\n<br>\n\n<center>\n    <figure>\n        <img src=\"https://i.postimg.cc/wj0FdNkN/CCT.png\" alt =\"CCT (Diagram)\" style='width:100%;'>\n        <figcaption>\n            Image Source: <a href=\"https://paperswithcode.com/method/cct\">PapersWithCode | Compact Convolutional Transformers</a>\n        </figcaption>\n    </figure>\n</center>\n\n<br>\n\nAt its core, CCT integrates convolutional layers in the tokenization step, which excel at extracting spatial hierarchies and local features, with self-attention mechanisms intrinsic to Transformers, which enable modelling global relationships in data. The result is a highly versatile architecture capable of simultaneously handling fine-grained details and holistic context in various tasks, such as image classification, object detection, and sequential data analysis.","metadata":{}},{"cell_type":"code","source":"from IPython.display import HTML\n\nHTML('<div align=\"center\"><iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/AEWhf_hMBgs\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe></div>')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-17T10:30:17.811059Z","iopub.execute_input":"2023-09-17T10:30:17.811452Z","iopub.status.idle":"2023-09-17T10:30:17.824212Z","shell.execute_reply.started":"2023-09-17T10:30:17.811415Z","shell.execute_reply":"2023-09-17T10:30:17.82324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The CCT architecture aims to increase performance and provide flexibility for input image sizes whilst demonstrating the independence of Positional Embedding in contrast to other transformer-based architectures. The utilization of the CCT architecture empowers researchers to achieve noteworthy results despite having restricted trainable parameters, small datasets, and a single processing unit, be it a GPU or a CPU.\n\nIn this section, we will cover the implementation and training of the Compact Convolutional Transformer to classify skin cancer with our pre-contructed input datapipeline.\n\n\n**For more information on the Compact Convolutional Transformer architectures and Transformers in general, please refer to the following links:**\n> - CCT Paper: [Escaping the Big Data Paradigm with Compact Transformers](https://arxiv.org/abs/2104.05704v4)\n> - Vision Transformer Paper: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2021)](https://arxiv.org/pdf/2010.11929.pdf)\n> - Attention Mechanism Paper: [Attention Is All You Need (2017)](https://arxiv.org/pdf/1706.03762.pdf)\n> - V7Labs Article: [Vision Transformer: What It Is & How It Works [2023 Guide]](https://www.v7labs.com/blog/vision-transformer-guide)\n> - Viso.ai Article: [Vision Transformers (ViT) in Image Recognition ‚Äì 2022 Guide](https://viso.ai/deep-learning/vision-transformer-vit/)","metadata":{}},{"cell_type":"code","source":"class CCTTokenizer(layers.Layer):\n    def __init__(\n        self,\n        kernel_size=3,\n        stride=1,\n        padding=1,\n        pooling_kernel_size=3,\n        pooling_stride=2,\n        num_conv_layers=2,\n        num_output_channels=[64, 128],\n        positional_embedding=True,\n        **kwargs,\n    ):\n        # Initialize tokenizer\n        super(CCTTokenizer, self).__init__(**kwargs)\n        self.tokenizer = Sequential()\n        \n        for _ in range(num_conv_layers):\n            # Add Conv Layer\n            self.tokenizer.add(\n                layers.Conv2D(\n                    num_output_channels[_],\n                    kernel_size, stride,\n                    padding=\"valid\", use_bias=False, \n                    activation=\"relu\",kernel_initializer=\"he_normal\",\n                    name=f'tokenizer_conv_{_}'\n                )\n            )\n            # Add Padding Layer\n            self.tokenizer.add(\n                layers.ZeroPadding2D(padding, name=f'tokenizer_padding_{_}')\n            )\n            \n            # Add MaxPool layer\n            self.tokenizer.add(\n                layers.MaxPool2D(\n                    pooling_kernel_size, \n                    pooling_stride, \"same\", \n                    name=f'tokenizer_maxpool_2d_{_}'\n                )\n            )\n\n        self.positional_embedding = positional_embedding\n\n    def __call__(self, images):\n        # Tokenize image\n        tokenized_image = self.tokenizer(images)\n        \n        # Flatten spatial dimensions to form sequences\n        img_seq = tf.reshape(\n            tokenized_image,\n            (-1, tf.shape(tokenized_image)[1] * tf.shape(tokenized_image)[2], tf.shape(tokenized_image)[-1]),\n        )\n        \n        return img_seq\n\n    def gen_positional_embedding(self, image_size):\n        # Optional: Positional embedding\n        if self.positional_embedding:\n            # Compute number of sequences to initialise an embedding layer\n            # used to compute the positional embeddings.\n            dummy_inputs = tf.ones((1, image_size, image_size, 3)) # dummy input\n            \n            # Pass input to tokenizer\n            dummy_outputs = self.__call__(dummy_inputs)\n            \n            # Get seq length and projection dimension from dummy output\n            sequence_length = dummy_outputs.shape[1]\n            projection_dim = dummy_outputs.shape[-1]\n\n            # Generate an embedding layer\n            embedding_layer = layers.Embedding(\n                input_dim=sequence_length, output_dim=projection_dim, name='tokenizer_pos_embedding'\n            )\n            \n            return embedding_layer, sequence_length\n        else:\n            return None # Positional Embedding Disabled","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:30:17.82699Z","iopub.execute_input":"2023-09-17T10:30:17.830589Z","iopub.status.idle":"2023-09-17T10:30:17.851828Z","shell.execute_reply.started":"2023-09-17T10:30:17.830534Z","shell.execute_reply":"2023-09-17T10:30:17.850803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=3.1></a>\n## Stochastic Depth\n\nStochastic Depth is a regularization technique which is used to enhance the training of deep neural network architectures, especially within architectures which utillize skip connections. The key concept behind Stochastic Depth is tho randomly drop (or skip) entire residual blocks or layers during training, hence, effectively incorperating a form of controlled network pruning during each feedforward pass.\n\nThis regularization technique aims to prevent overfitting, encourage feature reusability, and improve the capability of model generalization. The usage of Stochastic Depth for the training of deep neural network models leads to substancially reduced training times, as well as increased generalization on test sample.\n\nTo train our CCT model, we'll make use of Stochatic Depth in persuit of increased performance.\n\n<br>\n\n**For more information on Stochastic Depth, kindly refer to the following links:**\n> - [Paper: Deep Networks with Stochastic Depth](https://arxiv.org/abs/1603.09382)\n> - [Medium.com | Review: Stochastic Depth (Image Classification)](https://towardsdatascience.com/review-stochastic-depth-image-classification-a4e225807f4a)\n> - [TensorFlow Addons (TFA) | Stochastic Depth](https://www.tensorflow.org/addons/api_docs/python/tfa/layers/StochasticDepth) ","metadata":{}},{"cell_type":"code","source":"class StochasticDepth(layers.Layer):\n    def __init__(self, droupout_rate, **kwargs):\n        super().__init__(**kwargs)\n        self.droupout_rate = droupout_rate\n\n    def __call__(self, x, training=None):\n        if training:\n            # Set survival rate\n            survival_rate = 1 - self.droupout_rate\n            \n            # Define output shape\n            shape = (x.shape[0],) + (1,) * (x.shape[0] - 1)\n            \n            # Generate random dropouts\n            random_tensor = survival_rate + tf.random.uniform(shape, 0, 1)\n            random_tensor = tf.floor(random_tensor)\n            \n            return (x / survival_rate) * random_tensor\n        \n        # Return input if not training \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:30:17.856198Z","iopub.execute_input":"2023-09-17T10:30:17.857161Z","iopub.status.idle":"2023-09-17T10:30:17.865493Z","shell.execute_reply.started":"2023-09-17T10:30:17.857128Z","shell.execute_reply":"2023-09-17T10:30:17.864519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=3.2></a>\n## Multi-Layer Perceptron (MLP)\n\n> For more information on Multi-Layer Perceptron (MLP), kindly refer to the following links:\n> - [Multilayer perceptrons](http://users.ics.aalto.fi/ahonkela/dippa/node41.html)\n> - [Activation Function| Gaussian Error Linear Units (GELUs)](https://arxiv.org/abs/1606.08415v5)\n> - [Dive Into Deep Learning | Multilayer Perceptrons](https://d2l.ai/chapter_multilayer-perceptrons/mlp.html) ","metadata":{}},{"cell_type":"code","source":"def mlp(x, hidden_units, dropout_rate):\n    for units in hidden_units:\n        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n        x = layers.Dropout(dropout_rate)(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:30:17.866983Z","iopub.execute_input":"2023-09-17T10:30:17.867671Z","iopub.status.idle":"2023-09-17T10:30:17.874221Z","shell.execute_reply.started":"2023-09-17T10:30:17.867483Z","shell.execute_reply":"2023-09-17T10:30:17.873186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=3.3></a>\n## Generate CCT Model","metadata":{}},{"cell_type":"code","source":"def create_cct_model(\n    image_size=CFG.IMAGE_SIZE,\n    input_shape=CFG.IMAGE_SHAPE,\n    num_heads=CFG.NUM_HEADS,\n    projection_dim=CFG.PROJECTION_DIM,\n    transformer_units=CFG.TRANSFORMER_UNITS,\n    transformer_layers=CFG.TRANSFORMER_LAYERS,\n    possitional_embedding=CFG.POSITIONAL_EMBEDDING,\n    stochastic_depth_rate = CFG.STOCHASTIC_DEPTH_RATE\n):\n    # Define Input Layer\n    inputs = layers.Input(input_shape, dtype=tf.float32, name='input_image')\n\n    # Augment input image\n    augmented = augmentation_layer(inputs)\n\n    # Tokenize image patches\n    cct_tokenizer = CCTTokenizer()\n    encoded_patches = cct_tokenizer(augmented)\n\n    # Apply positional embedding to tokens\n    if possitional_embedding:\n        pos_embedding_layer, seq_length = cct_tokenizer.gen_positional_embedding(image_size)\n        positions = tf.range(start=0, limit=seq_length, delta=1)\n        \n        pos_embeddings = pos_embedding_layer(positions)        \n        encoded_patches += pos_embeddings\n\n    # Compute Stochastic Depth probabilities\n    dpr = [_ for _ in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n\n    # Generate layers for the Transformer block\n    for _ in range(transformer_layers):\n        # Layer normalization\n        norm_layer_1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n\n        # Generate Multi-Head Self-Attention layer\n        attention_output = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n        )(norm_layer_1, norm_layer_1)\n\n        # Skip connection 1\n        attention_output = StochasticDepth(dpr[_])(attention_output)\n        residual_1 = layers.Add()([\n            attention_output, encoded_patches\n        ])\n\n        # Layer normalization\n        norm_layer_2 = layers.LayerNormalization(epsilon=1e-5)(residual_1)\n\n        # MLP Layer\n        mlp_layer = mlp(\n            norm_layer_2, hidden_units=transformer_units, dropout_rate=0.1\n        )\n\n        # Skip connection 2\n        stochastic_depth_1 = StochasticDepth(dpr[_])(mlp_layer)\n        encoded_patches = layers.Add()([stochastic_depth_1, residual_1])\n\n    # Apply sequence pooling\n    seq_representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n    \n    attention_weights = tf.nn.softmax(layers.Dense(1)(seq_representation), axis=1)\n    \n    weighted_seq_representation = tf.matmul(\n        attention_weights, \n        seq_representation, \n        transpose_a=True\n    )\n    weighted_seq_representation = tf.squeeze(weighted_seq_representation, -2)\n\n    # Feed forward to output layer\n    output_layer = layers.Dense(\n        CFG.NUM_CLASSES, \n        activation=tf.nn.sigmoid,\n        name='output_layer'\n    )(weighted_seq_representation)\n    \n    # Generate Model\n    model = tf.keras.Model(\n        inputs=[inputs], outputs=[output_layer], \n        name='compact_convolutional_transformer_model'\n    )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:30:17.875885Z","iopub.execute_input":"2023-09-17T10:30:17.876531Z","iopub.status.idle":"2023-09-17T10:30:17.893983Z","shell.execute_reply.started":"2023-09-17T10:30:17.876487Z","shell.execute_reply":"2023-09-17T10:30:17.89306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(CFG.TF_SEED)\n\n# Construct Model\ncct_model = create_cct_model()\n\n# View model summary\ncct_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:30:17.901866Z","iopub.execute_input":"2023-09-17T10:30:17.902532Z","iopub.status.idle":"2023-09-17T10:30:22.171093Z","shell.execute_reply.started":"2023-09-17T10:30:17.902486Z","shell.execute_reply":"2023-09-17T10:30:22.170323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n    <h3>Observe</h3>\n    We see that our <b>CCT Model consists of ~0.73M total parameters</b>. This is <b>astronomically less</b> than that of the <b>Vision Transformer (~86.0M parameters)</b> and <b>EfficientNet V2 (~6.0M parameters)</b> model variants.\n</div>","metadata":{}},{"cell_type":"code","source":"# Explore model visually\nplot_model(\n    cct_model, dpi=60,\n    show_shapes=True\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:30:22.172403Z","iopub.execute_input":"2023-09-17T10:30:22.172752Z","iopub.status.idle":"2023-09-17T10:30:22.735069Z","shell.execute_reply.started":"2023-09-17T10:30:22.172718Z","shell.execute_reply":"2023-09-17T10:30:22.733472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center><div style='color:#ffffff;\n           display:inline-block;\n           padding: 5px 5px 5px 5px;\n           border-radius:5px;\n           background-color:#B6EADA;\n           font-size:100%;'><a href=#toc style='text-decoration: none; color:#03001C;'>‚¨ÜÔ∏è Back To Top</a></div></center>\n\n<a id=4></a>\n# 4 | Training the Compact Convolutional Transformers\n<div style=\"padding: 4px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/V6fJnWTt/Ali.png); background-size: 100% auto;\"></div>\n\n<br><br>\n\n<center>\n    <figure>\n        <img src=\"https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExMzYzMzU3OWE2NmQwOTg3MzZmNGJjNWZiZjRjOTAwMDVmMGMxNGVlZiZlcD12MV9pbnRlcm5hbF9naWZzX2dpZklkJmN0PWc/PjJ1cLHqLEveXysGDB/giphy.gif\" alt =\"UCF101\" style='width: 60%;'>\n        <figcaption>\n            Source: Giphy | <a href=\"https://giphy.com/gifs/digital-brain-ai-PjJ1cLHqLEveXysGDB\">Glow Machine Learning</a> by <a href=\"https://giphy.com/xponentialdesign\">xponentialdesign</a>\n        </figcaption>\n    </figure>\n</center>\n\n<br>\n\nTo train the CCT model we'll use Binary Crossentropy as the loss function since this is a classification problem for binary labels. As for the optimizer, we'll use the Adam optimizer with 0.001 as the (default) learning rate. \n\nTo prevent the occurance of overfitting during training we'll have to make use of TensorFlow's Callback API to implement the EarlyStopping & (optional) ReduceLROnPlateau callbacks. The only metrics we'll track during the training of the model will be the loss and accuracy metrics.\n\n**See the following for more information:**\n>- **Binary Crossentropy Loss Function:**\n>    - [Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names](https://gombru.github.io/2018/05/23/cross_entropy_loss/)\n>    - [TensorFlow Binary Crossentropy Loss Implementation](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy)\n>- **AdamW Optimizer:**\n>    - [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)\n>    - [AdamW: Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101v3)\n>    - [TensorFlow Adam Implementation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)\n>    - [TensorFlow Addons AdamW Implementation](https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/AdamW)\n>- **TensorFlow Callback API:**\n>    - [EarlyStopping Implementation](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)\n>    - [ReduceLROnPlateau Implementation](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau)\n>- **TensorFlow Metrics:**\n>    - [TensorFlow Metrics Overview](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)","metadata":{}},{"cell_type":"code","source":"# Define Early Stopping Callback\nearly_stopping_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', \n    patience=20, \n    restore_best_weights=True)\n\n# (Optional) Define Reduce Learning Rate Callback\n# reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n#     monitor='val_loss',\n#     patience=6,\n#     factor=0.1,\n#     verbose=1)\n\n# Define Callbacks and Metrics lists\nCALLBACKS = [\n    early_stopping_callback, \n#     reduce_lr_callback\n]\n\nMETRICS = ['accuracy']","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:30:22.736495Z","iopub.execute_input":"2023-09-17T10:30:22.737596Z","iopub.status.idle":"2023-09-17T10:30:22.74319Z","shell.execute_reply.started":"2023-09-17T10:30:22.737557Z","shell.execute_reply":"2023-09-17T10:30:22.742208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(CFG.TF_SEED)\n\n# Compile the model\ncct_model.compile(\n    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1),\n    optimizer=tfa.optimizers.AdamW(learning_rate=CFG.LEARNING_RATE, weight_decay=CFG.WEIGHT_DECAY),\n    metrics=METRICS\n)\n\n# Train the model \nprint(f'Training {cct_model.name}.')\nprint(f'Train on {len(train_new_df)} samples, validate on {len(val_df)} samples.')\nprint('----------------------------------')\n\ncct_model_history = cct_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    batch_size=CFG.BATCH_SIZE,\n    epochs=CFG.EPOCHS,\n    callbacks=CALLBACKS,\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T10:30:22.744464Z","iopub.execute_input":"2023-09-17T10:30:22.74545Z","iopub.status.idle":"2023-09-17T11:17:34.023257Z","shell.execute_reply.started":"2023-09-17T10:30:22.745416Z","shell.execute_reply":"2023-09-17T11:17:34.021578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model\ncct_evaluation = cct_model.evaluate(test_ds)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T11:17:34.025291Z","iopub.execute_input":"2023-09-17T11:17:34.025985Z","iopub.status.idle":"2023-09-17T11:17:38.094382Z","shell.execute_reply.started":"2023-09-17T11:17:34.02595Z","shell.execute_reply":"2023-09-17T11:17:38.093439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate model probabilities and associated predictions\ncct_test_probabilities = cct_model.predict(test_ds, verbose=1)\ncct_test_predictions = tf.argmax(cct_test_probabilities, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T11:17:38.09591Z","iopub.execute_input":"2023-09-17T11:17:38.096226Z","iopub.status.idle":"2023-09-17T11:17:41.079661Z","shell.execute_reply.started":"2023-09-17T11:17:38.096202Z","shell.execute_reply":"2023-09-17T11:17:41.078719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n    <h3>Observe</h3>\n    We see that our CCT Model does indeed converge to a decent loss on the train, validation and test sets. However, there seems to be a noticable gap between the validation and test losses. \n</div>","metadata":{}},{"cell_type":"markdown","source":"<center><div style='color:#ffffff;\n           display:inline-block;\n           padding: 5px 5px 5px 5px;\n           border-radius:5px;\n           background-color:#B6EADA;\n           font-size:100%;'><a href=#toc style='text-decoration: none; color:#03001C;'>‚¨ÜÔ∏è Back To Top</a></div></center>\n\n<a id='5'></a>\n\n# 5 | Performance Evaluation\n<div style=\"padding: 4px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/V6fJnWTt/Ali.png); background-size: 100% auto;\"></div>\n\n<br>\n\nNow that the model has trained on the data we need to inspect how well it performs on the unseen test data. In order to conduct this inspection we need to evaluate the performance of the model on the test data and record evaluation metrics. Since this is a binary classification problem we'll make use of some well known classification metrics. Hence, we'll make use of the Scikit Learn library to inspect the model. We'll also use the following to inspect the model:\n\n> - [Classification Report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report)\n> - [Accuracy Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)\n> - [Precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score)\n> - [Recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score)\n> - [F1-score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) \n> - [Matthews Correlation Coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)","metadata":{}},{"cell_type":"markdown","source":"<a id='5.1'></a>\n### Plot Model Histories ","metadata":{}},{"cell_type":"code","source":"def plot_training_curves(history, ):\n    \n    loss = np.array(history.history['loss'])\n    val_loss = np.array(history.history['val_loss'])\n\n    accuracy = np.array(history.history['accuracy'])\n    val_accuracy = np.array(history.history['val_accuracy'])\n\n    epochs = range(len(history.history['loss']))\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n\n    # Plot loss\n    ax1.plot(epochs, loss, label='training_loss', marker='o')\n    ax1.plot(epochs, val_loss, label='val_loss', marker='o')\n    \n    ax1.fill_between(epochs, loss, val_loss, where=(loss > val_loss), color='C0', alpha=0.3, interpolate=True)\n    ax1.fill_between(epochs, loss, val_loss, where=(loss < val_loss), color='C1', alpha=0.3, interpolate=True)\n\n    ax1.set_title('Loss (Lower Means Better)', fontsize=16)\n    ax1.set_xlabel('Epochs', fontsize=12)\n    ax1.legend()\n\n    # Plot accuracy\n    ax2.plot(epochs, accuracy, label='training_accuracy', marker='o')\n    ax2.plot(epochs, val_accuracy, label='val_accuracy', marker='o')\n    \n    ax2.fill_between(epochs, accuracy, val_accuracy, where=(accuracy > val_accuracy), color='C0', alpha=0.3, interpolate=True)\n    ax2.fill_between(epochs, accuracy, val_accuracy, where=(accuracy < val_accuracy), color='C1', alpha=0.3, interpolate=True)\n\n    ax2.set_title('Accuracy (Higher Means Better)', fontsize=16)\n    ax2.set_xlabel('Epochs', fontsize=12)\n    ax2.legend();\n    \n    sns.despine();","metadata":{"execution":{"iopub.status.busy":"2023-09-17T11:17:41.081016Z","iopub.execute_input":"2023-09-17T11:17:41.081358Z","iopub.status.idle":"2023-09-17T11:17:41.094397Z","shell.execute_reply.started":"2023-09-17T11:17:41.081324Z","shell.execute_reply":"2023-09-17T11:17:41.092497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot cct model training history \nplot_training_curves(cct_model_history)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T11:17:41.095897Z","iopub.execute_input":"2023-09-17T11:17:41.096731Z","iopub.status.idle":"2023-09-17T11:17:41.844353Z","shell.execute_reply.started":"2023-09-17T11:17:41.096695Z","shell.execute_reply":"2023-09-17T11:17:41.843455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n    <h3>Observe</h3>\n    Here we see that our CCT Model loss initially converges at a stable rate, and becomes more unstable as the model trains after ~15 epochs. Some overfitting might have occured, which, if so, would explain this behaviour in the loss convergence. \n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id='5.2'></a>\n### Plot Confusion Matrix","metadata":{}},{"cell_type":"code","source":"def plot_confusion_matrix(y_true, y_pred, classes='auto', figsize=(10, 10), text_size=12): \n    # Generate confusion matrix \n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Set plot size\n    plt.figure(figsize=figsize)\n\n    # Create confusion matrix heatmap\n    disp = sns.heatmap(\n        cm, annot=True, cmap='Greens',\n        annot_kws={\"size\": text_size}, fmt='g',\n        linewidths=0.1, linecolor='black', clip_on=False,\n        xticklabels=classes, yticklabels=classes)\n    \n    # Set title and axis labels\n    disp.set_title('Confusion Matrix', fontsize=24)\n    disp.set_xlabel('Predicted Label', fontsize=20) \n    disp.set_ylabel('True Label', fontsize=20)\n    plt.yticks(rotation=0) \n\n    # Plot confusion matrix\n    plt.show()\n    \n    return","metadata":{"execution":{"iopub.status.busy":"2023-09-17T11:17:41.845879Z","iopub.execute_input":"2023-09-17T11:17:41.84646Z","iopub.status.idle":"2023-09-17T11:17:41.855604Z","shell.execute_reply.started":"2023-09-17T11:17:41.846426Z","shell.execute_reply":"2023-09-17T11:17:41.854516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = ['malignant', 'benign']\n\nplot_confusion_matrix(\n    test_df.label_encoded, \n    cct_test_predictions, \n    figsize=(6, 6), \n    classes=class_names)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T11:17:41.857016Z","iopub.execute_input":"2023-09-17T11:17:41.857887Z","iopub.status.idle":"2023-09-17T11:17:42.153256Z","shell.execute_reply.started":"2023-09-17T11:17:41.857852Z","shell.execute_reply":"2023-09-17T11:17:42.152332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n    <h3>Observe</h3>\n    We can see that the CCT Model is able to detect beningn cases much better in comparrison to malignant cases.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id='5.3'></a>\n### Plot ROC Curves\n\n<center>\n    <figure>\n        <img src=\"https://paulvanderlaken.files.wordpress.com/2019/08/roc.gif?w=800&h=400&crop=1\" alt =\"AUC\" style='width:75%;'>\n        <figcaption>\n            Source: <a href=\"https://paulvanderlaken.com/2019/08/16/roc-auc-precision-and-recall-visually-explained/\">ROC, AUC, precision, and recall visually explained</a></figcaption>\n    </figure>\n</center>\n\nAn ROC (Receiver Operating Characteristic) curve is a measure that illustrates the diagnostic ability of a classifier system as its discrimination threshold is varied. The curve is created by plotting the true positive rate (TPR) on the Y-axis against the false positive rate (FPR) on the X-axis at various classification thresholds. The area under the ROC curve (AUC) is calculated and used as a metric showing how well a model can classify data points.\n\n**For more information see the following:**\n> - [What is a ROC Curve, and How Do You Use It in Performance Modeling?](https://www.simplilearn.com/what-is-a-roc-curve-and-how-to-use-it-in-performance-modeling-article)\n> - [Receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n> - [Understanding AUC-ROC: Clearly explained](https://medium.datadriveninvestor.com/understanding-auc-roc-clearly-explained-74c53d292a02)\n> - [ROC, AUC, precision, and recall visually explained](https://paulvanderlaken.com/2019/08/16/roc-auc-precision-and-recall-visually-explained/)","metadata":{}},{"cell_type":"code","source":"plot_roc(test_df.label_encoded, \n         cct_test_probabilities, \n         figsize=(8, 8), title_fontsize='large');","metadata":{"execution":{"iopub.status.busy":"2023-09-17T11:17:42.154733Z","iopub.execute_input":"2023-09-17T11:17:42.155338Z","iopub.status.idle":"2023-09-17T11:17:42.589344Z","shell.execute_reply.started":"2023-09-17T11:17:42.155302Z","shell.execute_reply":"2023-09-17T11:17:42.588411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n    <h3>Observe</h3>\n    The CCT Model is able to achieve a decent ROC curve on both malignant and benign cases, which is an indication that the model is a strong classifier.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id='5.4'></a>\n### View Classification Report","metadata":{}},{"cell_type":"code","source":"print(classification_report(test_df.label_encoded, \n                            cct_test_predictions, \n                            target_names=class_names))","metadata":{"execution":{"iopub.status.busy":"2023-09-17T11:17:42.590957Z","iopub.execute_input":"2023-09-17T11:17:42.593814Z","iopub.status.idle":"2023-09-17T11:17:42.609379Z","shell.execute_reply.started":"2023-09-17T11:17:42.593775Z","shell.execute_reply":"2023-09-17T11:17:42.608401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5.5'></a>\n### Record Classification Metrics","metadata":{}},{"cell_type":"code","source":"def generate_preformance_scores(y_true, y_pred, y_probabilities):\n    \n    model_accuracy = accuracy_score(y_true, y_pred)\n    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, \n                                                                                 y_pred, \n                                                                                 average=\"weighted\")\n    model_matthews_corrcoef = matthews_corrcoef(y_true, y_pred)\n    \n    print('=============================================')\n    print(f'\\nPerformance Metrics:\\n')\n    print('=============================================')\n    print(f'accuracy_score:\\t\\t{model_accuracy:.4f}\\n')\n    print('_____________________________________________')\n    print(f'precision_score:\\t{model_precision:.4f}\\n')\n    print('_____________________________________________')\n    print(f'recall_score:\\t\\t{model_recall:.4f}\\n')\n    print('_____________________________________________')\n    print(f'f1_score:\\t\\t{model_f1:.4f}\\n')\n    print('_____________________________________________')\n    print(f'matthews_corrcoef:\\t{model_matthews_corrcoef:.4f}\\n')\n    print('=============================================')\n    \n    preformance_scores = {\n        'accuracy_score': model_accuracy,\n        'precision_score': model_precision,\n        'recall_score': model_recall,\n        'f1_score': model_f1,\n        'matthews_corrcoef': model_matthews_corrcoef\n    }\n    return preformance_scores\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T11:17:42.610831Z","iopub.execute_input":"2023-09-17T11:17:42.611171Z","iopub.status.idle":"2023-09-17T11:17:42.620923Z","shell.execute_reply.started":"2023-09-17T11:17:42.611138Z","shell.execute_reply":"2023-09-17T11:17:42.61986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Record CCT model performance scores\ncct_performance = generate_preformance_scores(test_df.label_encoded, \n                                              cct_test_predictions, \n                                              cct_test_probabilities)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T11:17:42.622663Z","iopub.execute_input":"2023-09-17T11:17:42.623008Z","iopub.status.idle":"2023-09-17T11:17:42.641629Z","shell.execute_reply.started":"2023-09-17T11:17:42.62296Z","shell.execute_reply":"2023-09-17T11:17:42.640504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n    <h3>Observe</h3>\n    Here we see that the CCT Model is able to achieve a good Matthew's Correlation Coefficient (MCC), which validates the strength of the model. An MCC closer to 1.0 in an indication that a models predictions are statistically of high quality and that the model is capable of predicting on unseen data.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<center><div style='color:#ffffff;\n           display:inline-block;\n           padding: 5px 5px 5px 5px;\n           border-radius:5px;\n           background-color:#B6EADA;\n           font-size:100%;'><a href=#toc style='text-decoration: none; color:#03001C;'>‚¨ÜÔ∏è Back To Top</a></div></center>\n           \n\n<a id='conclusion'></a>\n\n# <center>Conclusion</center>\n<div style=\"padding: 4px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/V6fJnWTt/Ali.png); background-size: 100% auto;\"></div>\n\n<br>\n\nIn this notebook we covered the implementation, training and performance analysis of a Compact Convolutional Transformer approach introduced in the paper \"Escaping the Big Data Paradigm with Compact Transformers\". We also briefly touched on other concepts such as Stochastic Depth and ROC Curves.\n\nWe found that not only did we manage to achieve substantially less total parameters in CCT in comaprison to other State-of-the-Art (SOTA) models such as the Vision Transformer and EfficientNet V2, we also managed to train the CCT model and achieved fantastic results on a single GPU without any transfer learning techniques.\n\nAs seen \n\n<h3>Suggestions for improving model performance</h3>\n\nIn order to improve the model's performance for skin classification, the following should be considered:\n<ul>\n<li><b>Increasing the input image size:</b> Due to being constrained with the ammount of memory available for this notebook, setting up a two gpu's or implementing a better data input pipeline may allow for larger image sizes to be used, which may allow for improved loss convergence and a higher MCC.</li>\n<br>\n<li><b>Hyperparameter Tuning:</b> By tuning the model's hyperameters with optimization frameworks such as <a href=\"https://optuna.org/\">Optuna</a>, we can increase the model's performance by using the optimal hyperparameters to train the model.</li>\n<br>    \n<li><b>Increasing the depth of the model:</b> Increasing the depth of the model by adding more transformer blocks or more convolutional layers to the tokenizer may lead to improved performance.</li>\n</ul>\n\n<br>\n\n<hr>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">  \n    <center><h3>Thank you for taking the time to view my notebook, and I hope you found this interesting!</h3></center>\n</div>","metadata":{}}]}